# NEWS/NEWS 任务报告

- agent: news/news
- requestId: 1772237943626-ifo2aj
- 生成时间(UTC): 2026-02-28T00:19:34.095Z

## 原文中英重写

## 中文重写
开放协同进化与自博弈是驱动生物复杂性与人工智能进展的核心机制。在生物学中，进化军备竞赛的红皇后动态被认为是产生复杂性的关键机制之一[44]。受此启发，研究者通过智能体种群间的协同进化在计算机中模拟这一过程[45]。例如，Polyworld[46]在模拟环境中进化由神经网络控制的智能体，它们竞争资源并发展出捕食和合作等行为；POET[47 48]则开放地协同进化智能体及其环境，自动生成问题与解决方案。许多其他研究也将协同进化作为获取复杂性的主要机制[49 50 51 52 53]。为稳定进化算法，质量多样性算法通过维持多样性被提出[54 55 29 56]。在强化学习中，红皇后动态以自博弈形式体现：智能体在对手为其自身历史副本或相关实体的环境中训练[30]。自博弈推动了AI的重大突破，从早期的跳棋[57]和双陆棋[58]到现代棋类游戏如围棋[59 60]、国际象棋[61]及3D多智能体环境[62 63]，并成为掌握复杂实时多人策略游戏如《星际争霸II》[64 65]和《Dota 2》[66]的关键因素。近期研究还表明自博弈能生成稳健的自动驾驶策略[67]。自博弈可视为自动课程学习的一种形式[68]，抽象为一个智能体与环境交互，另一个智能体生成环境[69]。在此框架下，我们的DRQ算法与虚构自博弈(FSP)[70 27 71]和策略空间响应预言机(PSRO)[28]密切相关，后者提供了多智能体学习的博弈论框架。FSP通过对手历史策略的经验平均来训练近似最优响应；PSRO则迭代扩展策略种群，通过对现有策略混合物的近似最优响应训练，并求解元游戏计算策略种群的纳什均衡分布。相比之下，DRQ不构建显式元策略或求解元游戏，而是直接在包含所有先前智能体的多智能体环境中优化当前智能体。此外，由于我们的领域缺乏定义良好的动作空间，我们在内循环中采用进化算法优化智能体，使方法超越标准基于动作的游戏设置。最后，我们使用大语言模型(LLMs)指导进化优化过程。

## English Rewrite
Open-ended coevolution and self-play are core mechanisms driving complexity in biology and advances in artificial intelligence. In biology, the Red Queen dynamics of evolutionary arms races are considered a key mechanism for generating complexity [44]. Inspired by this, researchers have sought to capture the mechanism in silico through coevolution between populations of agents [45]. For instance, Polyworld [46] evolves neural-network-controlled agents in a simulated environment where they compete for resources and develop behaviors such as predation and cooperation; POET [47 48] open-endedly co-evolves agents and their environments, automatically generating problems and solutions. Many other works feature coevolution as a primary mechanism for achieving complexity [49 50 51 52 53]. Quality-diversity algorithms have been proposed to stabilize evolutionary algorithms by maintaining diversity [54 55 29 56]. In reinforcement learning, the Red Queen dynamics manifest as self-play, where agents are trained in environments where the opponent is a historical copy of themselves or related in some way [30]. Self-play has driven some of AI's most significant breakthroughs, from early successes in checkers [57] and backgammon [58] to modern advances in board games like Go [59 60], chess [61], and 3D multi-agent environments [62 63]. It has also been pivotal in mastering complex real-time multiplayer strategy games such as StarCraft II [64 65] and Dota 2 [66]. Recent work shows self-play can produce robust self-driving car policies [67]. Self-play can be viewed as a form of automatic curriculum learning [68] and abstracted as one agent interacting with the environment while another generates it [69]. Within this framework, our DRQ algorithm is closely related to Fictitious Self-Play (FSP) [70 27 71] and Policy Space Response Oracles (PSRO) [28], which provide game-theoretic frameworks for multi-agent learning. FSP trains agents by learning approximate best responses to the empirical average of their opponents' past policies. PSRO iteratively expands a population of policies by training approximate best responses to mixtures of existing strategies and solving a meta-game to compute Nash equilibrium distributions over the strategy population. In contrast, DRQ does not construct explicit meta-strategies or solve a meta-game; instead, we directly optimize the current agent within a multi-agent environment containing all previous agents. Moreover, because our domain lacks a well-defined action space, we employ an evolutionary algorithm in the inner loop to optimize agents, allowing our approach to extend beyond standard action-based game settings. Finally, we use large language models (LLMs) to guide the evolutionary optimization process.
